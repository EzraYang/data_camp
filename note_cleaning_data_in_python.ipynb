{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Cleaning-Data-in-Python\" data-toc-modified-id=\"Cleaning-Data-in-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning Data in Python</a></div><div class=\"lev2 toc-item\"><a href=\"#1-exploring-your-data\" data-toc-modified-id=\"1-exploring-your-data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>1 exploring your data</a></div><div class=\"lev3 toc-item\"><a href=\"#1.1-diagnose-data-for-cleaning\" data-toc-modified-id=\"1.1-diagnose-data-for-cleaning-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>1.1 diagnose data for cleaning</a></div><div class=\"lev3 toc-item\"><a href=\"#1.2-exploratory-data-analysis\" data-toc-modified-id=\"1.2-exploratory-data-analysis-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>1.2 exploratory data analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#1.3-Visual-exploratory-data-analysis\" data-toc-modified-id=\"1.3-Visual-exploratory-data-analysis-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>1.3 Visual exploratory data analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#2-Tidying-data-for-analysis\" data-toc-modified-id=\"2-Tidying-data-for-analysis-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>2 Tidying data for analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#2.1-Tidy-data\" data-toc-modified-id=\"2.1-Tidy-data-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>2.1 Tidy data</a></div><div class=\"lev3 toc-item\"><a href=\"#2.2-Pivoting-data\" data-toc-modified-id=\"2.2-Pivoting-data-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>2.2 Pivoting data</a></div><div class=\"lev3 toc-item\"><a href=\"#2.3-beyond-melt-and-pivot\" data-toc-modified-id=\"2.3-beyond-melt-and-pivot-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>2.3 beyond melt and pivot</a></div><div class=\"lev2 toc-item\"><a href=\"#3-Combining-data-for-analysis\" data-toc-modified-id=\"3-Combining-data-for-analysis-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>3 Combining data for analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#3.1-concatenating-data\" data-toc-modified-id=\"3.1-concatenating-data-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>3.1 concatenating data</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2-finding-and-concatenating-data\" data-toc-modified-id=\"3.2-finding-and-concatenating-data-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>3.2 finding and concatenating data</a></div><div class=\"lev3 toc-item\"><a href=\"#3.3-merge-data\" data-toc-modified-id=\"3.3-merge-data-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>3.3 merge data</a></div><div class=\"lev2 toc-item\"><a href=\"#4-cleaning-data-for-analysis\" data-toc-modified-id=\"4-cleaning-data-for-analysis-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>4 cleaning data for analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#4.1-data-type\" data-toc-modified-id=\"4.1-data-type-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>4.1 data type</a></div><div class=\"lev3 toc-item\"><a href=\"#4.2-Using-regular-expressions-to-clean-strings\" data-toc-modified-id=\"4.2-Using-regular-expressions-to-clean-strings-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>4.2 Using regular expressions to clean strings</a></div><div class=\"lev3 toc-item\"><a href=\"#4.3-using-functions-to-clean-data\" data-toc-modified-id=\"4.3-using-functions-to-clean-data-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>4.3 using functions to clean data</a></div><div class=\"lev3 toc-item\"><a href=\"#4.4-duplicate-and-missing-data\" data-toc-modified-id=\"4.4-duplicate-and-missing-data-144\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>4.4 duplicate and missing data</a></div><div class=\"lev3 toc-item\"><a href=\"#4.5-testing-with-asserts\" data-toc-modified-id=\"4.5-testing-with-asserts-145\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>4.5 testing with asserts</a></div><div class=\"lev2 toc-item\"><a href=\"#5-case-study\" data-toc-modified-id=\"5-case-study-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>5 case study</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n",
    "* 感觉这个课更多是讲清理的思路，where to clean, what to clean\n",
    "* 而pandas系列课程偏重学会各种function\n",
    "\n",
    "content  \n",
    "* Exploring your data\n",
    "  * Diagnose data for cleaning\n",
    "  * Exploratory data analysis\n",
    "  * Visual exploratory data analysis\n",
    "* Tidying data for analysis ----------------- ☆\n",
    "\t* Tidy data\n",
    "\t* Pivoting data\n",
    "\t* Beyond melt and pivot\n",
    "* Combining data for analysis\n",
    "\t* Concatenating data\n",
    "\t* Finding and concatenating data ---- ☆\n",
    "\t* Merge data\n",
    "* Cleaning data for analysis\n",
    "\t* Data types\n",
    "\t* Using regular expressions to clean strings ---- ☆\n",
    "\t* Using functions to clean data ------------------ ☆☆\n",
    "\t* Duplicate and missing data ---------------------- ☆\n",
    "\t* Testing with asserts\n",
    "* Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 exploring your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 diagnose data for cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common data problems\n",
    "* inconsistent column names \n",
    "* missing data \n",
    "* outliers\n",
    "* duplicate rows\n",
    "* untidy\n",
    "* need to process columns \n",
    "* column types can signal unexpected data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# think about column names and missing vals \n",
    "df.head()\n",
    "df.tail()\n",
    "df.columns()\n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# droping unnecessary cols\n",
    "df_dropped = df.drop([cols_to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 exploratory data analysis\n",
    "* to help identify data that needs further investigation\n",
    "    * count the number of unique vals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-numeric cols: frequency count to spot NaN\n",
    "df.continent.value_counts(dropna=False)   # count also the missing vals\n",
    "\n",
    "# numeric cols: summary stats to spot outlier\n",
    "df.describe()  # only results of num cols returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visual exploratory data analysis\n",
    "when there's too many cols, summary stats alone can be overwhelming  \n",
    "use visual aid to :  \n",
    "   * spot outliers and obvious errors\n",
    "   * look for pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plots and histograms\n",
    "    # bar plots for deicrete data counts\n",
    "    # hist for continuous data counts\n",
    "        * df.colA.plot(kind='hist')\n",
    "        * df[df.population > 1000000000]   # select a subset to see if some error\n",
    "    # is there any unexpected freq of vals?\n",
    "    \n",
    "# boxplots\n",
    "    # outliers, percentiles\n",
    "    # when you have a numeric column that you want to compare across different categories\n",
    "    * df.boxplot(column='population', by'continent')\n",
    "    \n",
    "# scatter plots\n",
    "    # relationship between 2 numeric vals\n",
    "    # potentially bad data\n",
    "        # eg. errors not found by looking at 1 val\n",
    "        # select a subset to see if some error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Tidying data for analysis\n",
    "### 2.1 Tidy data  \n",
    "* Principles of tidy data\n",
    "    * Columns represent separate variables \n",
    "    * Rows represent individual observations \n",
    "    * Observational units form tables（won't cover in this chap)\n",
    "* tidy data  \n",
    "    * better for analysis\n",
    "    * easier to fix common data problems  \n",
    "    * able to transfer data into diff shapes needed  \n",
    "* problem when tidying data\n",
    "    * columns containing values other than variables\n",
    "    * solution: pd.melt()  \n",
    "    \n",
    " (可是，为什么tidy data is a better format for analysis呢？)  \n",
    " 现在能想到的点是：方便groupby，方便画图(x=category_col, y=value, 加上一个aggregation func)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imagine this df\n",
    "df.columns = ['name', 'treatmentA', 'treatmentB']\n",
    "df.values[0] = [some_name, result_of_treA, result_of_treB] # this is vals in a row in df \n",
    "\n",
    "# 'treatmentA' and 'treatmentB' should be the value of 'treatment' col\n",
    "# pd.melt()\n",
    "pd.melt(df, \n",
    "        id_vars='name',          # the col to remain the same(idntical for other cols) \n",
    "        value_vars=['treatmentA', 'treatmentB'],   # the cols to melt\n",
    "                                                   # by default all cols other than id_vars\n",
    "        var_name='treatment',    # 'trA','trB'两栏被合并到一栏后，new col name\n",
    "        value_name='result'      # 从cell中释放出的vals成为新的一栏后，new col name\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pivoting data  \n",
    "* melt和pivot方向相反\n",
    "    * melting: turn cols into rows\n",
    "    * pivoting: turn unique vals into seperate cols \n",
    "* pivot用处\n",
    "    * analysis-friendly shape to reporing-friendly shape\n",
    "    * multiple vars stored in the same col ->> reshape into tidy\n",
    "* 我理解  \n",
    "    一个光谱，最左边<<-是最长的long format，最右->>是最宽的wide format  \n",
    "    tidy shape and data in hand both lies somewhere in between  \n",
    "    要向long form挪，<<-pd.melt()  \n",
    "    要向wide form挪，->>df.pivot() / df.pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in df.pivot, each index-columns pair should be unique\n",
    "df.pivot(index = cols_you_dont_want_to_reform, \n",
    "         columns= cols_you_want_each_unique_vals_has_a_sep_col,\n",
    "         values= col_you_want_to_pivot_in_cell  # each index-col pair responds one value\n",
    "        )\n",
    "\n",
    "# in df.pivot_table(), each index-columns pair can have multi vals\n",
    "df.pivot_table(index= ,\n",
    "               columns= ,\n",
    "               values= ,\n",
    "               aggfunc=    # to aggregate multi vals of one index-col pair\n",
    "                           # default mean\n",
    "              )\n",
    "\n",
    "# if there's multi-level index after .pivot() / .pivot_table() and you don't want it\n",
    "df.pivot(...).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 beyond melt and pivot\n",
    "这节只讲了一种情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the **str attribute** of columns of type object.\n",
    "\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]  # create a new col \n",
    "                                             # pandas' vectorized string slicing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Combining data for analysis\n",
    "### 3.1 concatenating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack data vertically\n",
    "pd.concat([df1, df2])  # keep original index in both df\n",
    "\n",
    "pd.concat([df1, df2], \n",
    "          ignore_index=True  # discard origin index, reindex with a rangeIndex\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 finding and concatenating data\n",
    "deals with the situation where you want to load too many files  \n",
    "and there's a pattern in file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globbing\n",
    "    # wildcards: * ?\n",
    "        # zero or more time of any char: *\n",
    "        # exactly one time of any char: ?\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob('*.csv')\n",
    "\n",
    "csv_files  # get a list of all files with name match the pattern\n",
    "\n",
    "lst_data = []\n",
    "\n",
    "for file_name in csv_files:\n",
    "    data = pd.read_csv(filename)\n",
    "    lst_data.append(data)       # will finally get a list of dataframes\n",
    "    \n",
    "pd.concat(lst_data)     # concat the list of dataframe into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ch1_slides.pdf', 'ch2_slides.pdf', 'ch3_slides.pdf', 'ch4_slides.pdf', 'ch5_slides.pdf'] \n",
      " ['note_pd1_pandas_foundations_note.ipynb', 'note_pd2_merging_df_with_pandas.ipynb', 'note_pd3_manipulating_df_with_pandas.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import glob \n",
    "pdf_files = glob.glob('*.pdf')\n",
    "jnb_files = glob.glob('note_*')\n",
    "print(pdf_files, '\\n', jnb_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 merge data\n",
    "详细内容见merging df with pandas course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(left=state_populations, right=state_codes,  \n",
    "          on=None, \n",
    "          left_on='state', right_on='name')  # by default an outer merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 cleaning data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in df['treatment b'], want object, get int\n",
    "type(df['treatment b']) # get int\n",
    "\n",
    "df['treatment b'] = df['treatment b'].astype(str)   # object dtype is encoded as strings\n",
    "                                                    # now df['treatment b'] should be of type object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols with limited unique levels, category dtype can save memory & make other operations faster\n",
    "df['sex'] = df['sex'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# want int, get string\n",
    "df['treatemnt a'].values  # get (['-', '12', '24'], dtype=object)\n",
    "\n",
    "# If you expect the data type of a column to be numeric (int or float), \n",
    "# but instead it is of type object, \n",
    "# this typically means that there is a non numeric value in the column, \n",
    "# which also signifies bad data.\n",
    "\n",
    "df['treatment a'] = pd.to_numeric(df['treatment a'],\n",
    "                                  errors='coerce' # converting '-' to num will cause error\n",
    "                                                  # tell pandas not to stop at error but use NaN\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using regular expressions to clean strings  \n",
    "pattern matching similiar to globbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 12345678901 \n",
    "\\d*   # decimal digits one or more times\n",
    "\n",
    "# $12345678901\n",
    "\\$\\d*   # literally a dollar sign and decimal digits one or more times\n",
    "\n",
    "# $12345678901.42 \n",
    "\\$\\d*\\.\\d*   # add a literally period sign .\n",
    "\n",
    "# $12345678901.42 \n",
    "\\$\\d*\\.\\d{2}   # specifying there's 2 decimal digits after period\n",
    "\n",
    "# $12345678901.99\n",
    "^\\$\\d*\\.\\d{2}$  # ^ indicates start of the str\n",
    "                # $ indicates end of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *: zero or more time\n",
    "# +: one or more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 6), match='$17.89'> \n",
      " True\n",
      "\n",
      "\n",
      "['$17.89', '$12345678901.99']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "\n",
    "result = pattern.match('$17.89')\n",
    "\n",
    "print(result, '\\n',bool(result))\n",
    "print('\\n')\n",
    "\n",
    "result2 = re.findall('\\$\\d*\\.\\d{2}',     # get a list of all matching string\n",
    "                     '$17.89 lalalla $12345678901.99 heyhey $1234')\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 补充一个类似的，python '{}'.format()   \n",
    "[Python String Format Cookbook](https://mkaz.tech/code/python-string-format-cookbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 using functions to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function along with .apply()\n",
    "\n",
    "# example code:\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "    \n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 duplicate and missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicate data\n",
    "df.drop_duplicate()  # drop exact same rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing data NaN\n",
    "    # leave as-is\n",
    "    # drop them\n",
    "    # fill missing vals\n",
    "    \n",
    "    # drop them\n",
    "df.info()  # only count non-null vals \n",
    "df.dropna()  # drop any row contain NaN\n",
    "\n",
    "    # fill missing vals\n",
    "tips_nan['sex'].fillna('missing')            # fill with a 'missing' indicator\n",
    "tips_nan[['total_bill', 'size']].fillna(0)   # fill with 0\n",
    "\n",
    "tips_nan['tip'].fillna(tips_nan['tip'].mean())  # fill with mean of the series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 testing with asserts  \n",
    "* programmatically checking if sth wrong\n",
    "* if we drop or fill NaNs, we expect 0 missing vals  \n",
    "write assert to verify this  \n",
    "  \n",
    "(不过没太懂assert有毛用，不加assert不一样有True | False 的return value吗）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the assertions evals True, nothing will happen\n",
    "assert 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7f0558622fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if the assertion evals False, AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if the assertion evals False, AssertionError\n",
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at data\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "df.columns\n",
    "df.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with duplicate and missing vals\n",
    "df.drop_duplicate()\n",
    "\n",
    "df.dropna()\n",
    "df.colA.fillna( df.colA.mean() )\n",
    "assert pd.notnull(df.colA).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dtype\n",
    "df.dtype\n",
    "\n",
    "df.colA = df.colA.to_numeric()\n",
    "df.colB = df.colB.astype(str)\n",
    "\n",
    "assert df.colA.dtype == np.int64\n",
    "assert df.colB.dtype == np.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean function and sanity check\n",
    "def clean_func(row_data):\n",
    "    # clean steps\n",
    "    return ...\n",
    "    \n",
    "df.apply(clean_func, axis=1) # clean by row\n",
    "\n",
    "# cleaning using regular expression  例子见上笔记\n",
    "\n",
    "assert (some_conds_on_df)  # check the clean process works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining data \n",
    "# clean first then combine, or vice versa\n",
    "pd.merge(df1, df2,...)\n",
    "pd.concat([df1, df2, df3,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pivot or melt to get tidy data\n",
    "pd.melt(df, id_vars= , value_vars= , var_name= , value_name= )\n",
    "df.pivot(index= , columns= , value= )\n",
    "df.pivot_table(index= , columns= , value= , aggfunc= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new col as needed\n",
    "df['new_column'] = df['column_1'] + df['column_2']\n",
    "df['new_column'] = df.apply(my_function, axis=1)\n",
    "\n",
    "# create new col using regular expression 例子见上笔记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the cleaned data\n",
    "df.to_csv['my_data.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "387px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
