{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Clustering-for-dataset-exploration\" data-toc-modified-id=\"Clustering-for-dataset-exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clustering for dataset exploration</a></div><div class=\"lev2 toc-item\"><a href=\"#Unsupervised-learning-with-KMeans\" data-toc-modified-id=\"Unsupervised-learning-with-KMeans-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Unsupervised learning with KMeans</a></div><div class=\"lev2 toc-item\"><a href=\"#Evaluating-a-clustering\" data-toc-modified-id=\"Evaluating-a-clustering-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Evaluating a clustering</a></div><div class=\"lev2 toc-item\"><a href=\"#Transforming-features-for-better-clusterings\" data-toc-modified-id=\"Transforming-features-for-better-clusterings-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Transforming features for better clusterings</a></div><div class=\"lev1 toc-item\"><a href=\"#Visualization-with-hierarchical-clustering-and-t-SNE\" data-toc-modified-id=\"Visualization-with-hierarchical-clustering-and-t-SNE-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Visualization with hierarchical clustering and t-SNE</a></div><div class=\"lev2 toc-item\"><a href=\"#Visualizing-hierarchies\" data-toc-modified-id=\"Visualizing-hierarchies-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Visualizing hierarchies</a></div><div class=\"lev2 toc-item\"><a href=\"#extract-labels-at-intermediate-stage\" data-toc-modified-id=\"extract-labels-at-intermediate-stage-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>extract labels at intermediate stage</a></div><div class=\"lev2 toc-item\"><a href=\"#t-SNE-for-2-dimensional-maps\" data-toc-modified-id=\"t-SNE-for-2-dimensional-maps-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>t-SNE for 2-dimensional maps</a></div><div class=\"lev1 toc-item\"><a href=\"#Decorrelating-data-and-dimension-reduction\" data-toc-modified-id=\"Decorrelating-data-and-dimension-reduction-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Decorrelating data and dimension reduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Intrinsic-dimension\" data-toc-modified-id=\"Intrinsic-dimension-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Intrinsic dimension</a></div><div class=\"lev2 toc-item\"><a href=\"#Dimension-reduction-with-PCA\" data-toc-modified-id=\"Dimension-reduction-with-PCA-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dimension reduction with PCA</a></div><div class=\"lev1 toc-item\"><a href=\"#Discovering-interpretable-features\" data-toc-modified-id=\"Discovering-interpretable-features-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Discovering interpretable features</a></div><div class=\"lev2 toc-item\"><a href=\"#Non-negative-matrix-factorization-(NMF)\" data-toc-modified-id=\"Non-negative-matrix-factorization-(NMF)-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Non-negative matrix factorization (NMF)</a></div><div class=\"lev2 toc-item\"><a href=\"#understand-NMF-components\" data-toc-modified-id=\"understand-NMF-components-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>understand NMF components</a></div><div class=\"lev2 toc-item\"><a href=\"#Building-recommender-systems-using-NMF\" data-toc-modified-id=\"Building-recommender-systems-using-NMF-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Building recommender systems using NMF</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* module summarize\n",
    "    * models\n",
    "      * from sklearn.cluster import KMeans\n",
    "      * from scipy.cluster.hierarchy import linkage, dendrogram, fcluster  \n",
    "      .  \n",
    "\n",
    "    * dimension reduction\n",
    "      * from sklearn.manifold import TSNE\n",
    "      * from sklearn.decomposition import PCA\n",
    "      * from sklearn.decomposition import TruncatedSVD\n",
    "      * from sklearn.decomposition import NMF  \n",
    "      .  \n",
    "      \n",
    "    * preprocessing\n",
    "      * from sklearn.preprocessing import StandardScaler\n",
    "      * from sklearn.preprocessing import Normalizer\n",
    "      * from sklearn.preprocessing import normalize  \n",
    "      .  \n",
    "\n",
    "    * others\n",
    "      * from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMeans clustring fit - predict\n",
    "    # (here, clustering without standardization)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "labels = model.predict(samples)\n",
    "\n",
    "model.cluster_centers_\n",
    "    # extract centroids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When there is species label to check against:  \n",
    "    * The goal here is to find if your algo. has clear-cutting edges.\n",
    "    * eg)   \n",
    "    Cluster1 is SpeciesA and SpeciesC,   \n",
    "    Cluster2 is SpeciesB and SpeciesE,   \n",
    "    Cluster3 is SpeciesD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a confusion-matrix-like obj. to see how good clst does    # once get true label, \n",
    "    # (if you somehow has the true label)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'lb_pred': label_pred, \n",
    "                   'lb_actual': label_actual})\n",
    "\n",
    "pd.crosstab(df['lb_pred'], df['lb_actual'])\n",
    "    # to build confusion-matrix-like obj. \n",
    "    # can also achieve by df.pivot_table(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When there's no 'specis' to check against\n",
    "    * A good clustering has tight clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.inertia_     # eval the clustering algo.\n",
    "    # Measures how spread out the clusters are \n",
    "    # within the same cluster (lower is better)\n",
    "    # k-means a empts to minimize the inertia when choosing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose n_clusters\n",
    "    * n_clusters gets bigger, inertia_ gets smaller (each cluster is tighter)\n",
    "    * trade-off between n_clusters and inertia_:   \n",
    "    want a small inertia_ but not too big n_clusters\n",
    "    * a rule of thumb is to choose the 'elbow point'  \n",
    "    ![](http://upload-images.jianshu.io/upload_images/1526845-f54736c17a0c0525.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming features for better clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scaling features(cols) with `StandardScaler`:  \n",
    "prevent the centroids being dominated by some dimensions.\n",
    "    * `StandardScaler`: each col has mean0 and var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature scaling using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(samples)\n",
    "\n",
    "samples_scaled = scaler.transform(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaling - fit - predict using pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "pipeline = make_pipeline(scaler, kmeans)  # get a pipeline obj.\n",
    "\n",
    "pipeline.fit(samples)\n",
    "labels = pipeline.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalizing samples(rows) with `Normalizer`  \n",
    "    * eg) Yahoo! Finance dataset:  \n",
    "    where each row corresponds to a company,   \n",
    "    and each column corresponds to a trading day.  \n",
    "    Some stocks are more expensive than others.  \n",
    "    `Normalizer()` rescales each sample - here, each company's stock price - independently of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalizing with Normalizerï¼ˆjust like StandardScaler)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "pipeline.fit(movements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with hierarchical clustering and t-SNE  \n",
    "\n",
    "In this chapter, you'll learn about two unsupervised learning techniques for data visualization, hierarchical clustering and t-SNE. Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compared to K-means, agglomerative algorithms are more cumbersome and do not scale well to large datasets. Agglomerative algorithms are more suitable for statistical studies. \n",
    "* These algorithms do offer the advantage of creating a complete range of nested cluster solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-learn implementation of agglomerative clustering **does not** offer the possibility of depicting a dendrogram from your data \n",
    "    * because such a visualization technique works fine with only a few cases, whereas you can expect to work on many examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hierarchical clustering\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "mergings = linkage(samples, method='complete')\n",
    "                           # method='single'\n",
    "\n",
    "dendrogram(mergings, \n",
    "           labels=country_names, \n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalizing and hierarchical clustering\n",
    "    # SciPy hierarchical clustering doesn't fit into a sklearn\n",
    "    # pipeline, so you'll need to use the normalize() function\n",
    "    # from sklearn.preprocessing instead of Normalizer.\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_movements = normalize(movements)\n",
    "\n",
    "mergings = linkage(normalized_movements, method='complete')\n",
    "\n",
    "dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract labels at intermediate stage  \n",
    "\n",
    "* for use in eg. cross-tabulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "labels = fcluster(mergings, \n",
    "                  15,    # extract labels at height 15 \n",
    "                         # in dendrogram \n",
    "                  criterion='distance')\n",
    "\n",
    "df = pd.DataFrame({'labels': labels, 'actal': actual_labels})\n",
    "\n",
    "pd.crosstab(df['labels'], df['actual'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE for 2-dimensional maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t-SNE\n",
    "    * maps samples to 2D or 3D space\n",
    "    * map approximately preserves nearness of samples\n",
    "    * great for inspecting datasets\n",
    "    * though get **different** mapping every time, clusters have **same** position relative to one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(learning_rate=100)\n",
    "    # try values between 50 and 200\n",
    "\n",
    "transformed = model.fit_transform(samples)\n",
    "    # simultaneously fits the model and transforms the data\n",
    "    # has no seperate fit() or transform() methods\n",
    "    # cant extend the map to include new data samples\n",
    "    # must start over each time\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "\n",
    "{\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()\n",
    "} or \n",
    "\n",
    "{\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "    # Annotate the points\n",
    "for x, y, actual_lb in zip(xs, ys, actual_lbs):\n",
    "    plt.annotate(actual_lb, (x, y), fontsize=5, alpha=0.75)\n",
    "plt.show()    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorrelating data and dimension reduction\n",
    "\n",
    "Dimension reduction summarizes a dataset using its common occuring patterns. In this chapter, you'll learn about the most fundamental of dimension reduction techniques, \"Principal Component Analysis\" (\"PCA\"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* demension rduction\n",
    "    * more efficient storage and computation\n",
    "    * remove less-informative 'noise' features,which cause problems for prediction tasks\n",
    "    * \"In many real-world applications, it's dimension reduction that makes prediction possible.\"   \n",
    "    \n",
    ".  \n",
    "A fundamental dimension reduction technique: `PCA(principal component analysis)`  \n",
    "* PCA: find 'pricipal components' along the direction of most variance\n",
    "    * first step, 'decorrelation' by \n",
    "        * rotation of axes,  \n",
    "        * shift data so each feature has mean 0 \n",
    "    * second step, reduces dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA()\n",
    "\n",
    "{\n",
    "model.fit(samples)\n",
    "    # learns the transformation from given data\n",
    "\n",
    "transformed = model.transform(samples)\n",
    "    # applies the learned transformation, \n",
    "    # can also be applied to new data\n",
    "    # no information lost, same nrows & ncols as origin\n",
    "    # PCA features does not correlate\n",
    "}\n",
    "or\n",
    "{\n",
    "model.fit_transform(samples)\n",
    "    # do fit and transform in one shot\n",
    "}\n",
    "    \n",
    "model.components_  \n",
    "    # extract 'principal conponents': \n",
    "    # the direction vector along which data varies most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `intrinsic dimension`: number of features needed to approximate the dataset\n",
    "    * can be detected with `PCA`\n",
    "    * `PCA` identifies intrinsic dimension when samples have *any number* of features\n",
    "    * `num. of intrinsic dimendion`   \n",
    "    = `num. of PCA features with` *`significant variance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting the variances of each Principal Component\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(samples)\n",
    "\n",
    "features = range(pca.n_components_)\n",
    "    # from 0 to num. of PC\n",
    "    \n",
    "plt.bar(features, pca.explained_variance_)\n",
    "    # variance of each PC, sorted descending\n",
    "    \n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the first Principal Component (as an arrow)\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "model = PCA()\n",
    "\n",
    "model.fit(grains)\n",
    "\n",
    "    # Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "    # Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "    # Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "    # Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "    # then select the PCA feature that has variance \n",
    "    # above arbitrary threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://upload-images.jianshu.io/upload_images/1526845-b1f2d994526a071f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dimension reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dimension reduction\n",
    "    * represents same data, using less features\n",
    "    * can be performed using `PCA`  \n",
    ".  \n",
    "* Dimension reduction with `PCA`\n",
    "    * assumes the high variance features are informative, low var features are 'noise'\n",
    "    * ![](http://upload-images.jianshu.io/upload_images/1526845-a74ab16916ebc03a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "    * specify how many feature to keep  \n",
    "    e.g. PCA(n_components=2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA dimension reduction of iris dataset\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(samples)\n",
    "transformed = pca.transform(samples)\n",
    "\n",
    "transformed.shape  # get (n_obs, 2)\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a PCA-like procedure on word frequency data\n",
    "    * data: \n",
    "        * rows: documents\n",
    "        * cols: word\n",
    "        * vals: frequencies\n",
    "    * scipy.sparse.csr_matrix obj. instead of np.array\n",
    "        * save space by remenbering only non-zero entries\n",
    "    * TruncatedSVD\n",
    "        * a PCA-like model that take csr_matrix as input,  \n",
    "          and output PCA-like feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "model = TruncatedSVD(n_components=3)\n",
    "\n",
    "model.fit(documents)  # `documents` is csr_matrix obj.\n",
    "\n",
    "transformed = model.transform(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering interpretable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NMF: non-negative matrix factorization\n",
    "    * dimension reduction technique\n",
    "    * **interpretable** ( compared to PCA) \n",
    "    * all sample features must **be non-negative**!\n",
    "    * use\n",
    "        * decomposits documets as combinations of common themes\n",
    "        * decomposits images to combinations of common pattern\n",
    "    * works with numpy array and csr_matrix  \n",
    ".  \n",
    "* 'tf-idf' measure presence of words in each document using 'tf-idf'\n",
    "    * 'tf':  frequency of word in document\n",
    "    * 'idf': reduces influence of freq word like 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=2) \n",
    "    # n_components must be specified when instantiation\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "nmf_features = model.transform(samples)\n",
    "\n",
    "model.components_\n",
    "    # like PCA, can extract components\n",
    "    # ncol of components = ncol of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## understand NMF components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF on documents\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=10)\n",
    "\n",
    "nmf.fit(articles) \n",
    "\n",
    "components_df = pd.DataFrame(nmf.components_, \n",
    "                             columns=words  # cols of original data\n",
    "                            )\n",
    "    # get nmf features (nmf components)\n",
    "\n",
    "row = components_df.iloc[3,:]  \n",
    "    # select the 4th feature\n",
    "\n",
    "row.nlargest()  \n",
    "    # a Series sort by vals in 4th feature\n",
    "    # that is, top frequent words in 4th feature \n",
    "    # (theme of 4th feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF on images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reconstruct a sample using NMF features \n",
    "# and NMF value of this sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://upload-images.jianshu.io/upload_images/1526845-216ea356e443cc98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building recommender systems using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "nmf = NMF(n_components=6)\n",
    "\n",
    "nmf_features = nmf.fit_transform(atricles)\n",
    "\n",
    "norm_features = normalize(nmf_features)\n",
    "df = pd.DataFrame(norm_features, \n",
    "                  index=titles  # article titles\n",
    "                 ) \n",
    "\n",
    "current_article = df.loc['Dog bites man']\n",
    "\n",
    "similarities = df.dot(current_article)\n",
    "    # df.dot(): matrix multiplication\n",
    "    \n",
    "similarities.nlargest()\n",
    "    # get a Series of article\n",
    "        # of which index is title\n",
    "        # value is similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on differences between np.dot() and np.inner(),  \n",
    "see [this post](https://stackoverflow.com/questions/11033573/difference-between-numpy-dot-and-inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "260px",
    "left": "28px",
    "right": "20px",
    "top": "110px",
    "width": "182px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
